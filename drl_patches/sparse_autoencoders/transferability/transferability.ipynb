{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4540a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-21 23:56:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGetting device.               \u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mcpu\u001b[0m\n",
      "\u001b[2m2025-04-21 23:56:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDevice                        \u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mcpu\u001b[0m\n",
      "\u001b[2m2025-04-21 23:56:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGetting device.               \u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mcpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from drl_patches.sparse_autoencoders.get_vectorizer import load_tfidf_vectorizer\n",
    "from drl_patches.sparse_autoencoders.getting_experiment_config import (\n",
    "    load_training_indexes,\n",
    ")\n",
    "import torch\n",
    "from drl_patches.logger import logger\n",
    "from drl_patches.sparse_autoencoders.classical_data_mining import get_metrics\n",
    "from drl_patches.sparse_autoencoders.utils import read_jsonl_file, set_seed\n",
    "from drl_patches.sparse_autoencoders.vulnerability_detection_features import (\n",
    "    get_diff_data, get_most_important_features, get_vuln_safe_data\n",
    ")\n",
    "set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c1c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    dataset_path: str\n",
    "    training_idx_path: str\n",
    "\n",
    "GBUG_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/gbug-java.csv\",\n",
    "    training_idx_path=\"../artifacts/gbug-java_train_indexes.json\",\n",
    ")\n",
    "\n",
    "DEFECT_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/defects4j.csv\",\n",
    "    training_idx_path=\"../artifacts/defects4j_train_indexes.json\",\n",
    ")\n",
    "\n",
    "HUMAN_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/humaneval.csv\",\n",
    "    training_idx_path=\"../artifacts/humaneval_train_indexes.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf0c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_testing_dataset(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Get the testing dataset path and the training indexes path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset.dataset_path)\n",
    "    with open( dataset.training_idx_path, \"r\") as f:\n",
    "        training_indices = json.load(f)\n",
    "    train_df = df.loc[training_indices]\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAE_ACTIVATIONS:\n",
    "    feature_diff_path: str\n",
    "    model_path: str\n",
    "    layer: int\n",
    "    top_k: int\n",
    "    dataset: Dataset\n",
    "    gbug_feature_safe_path: str\n",
    "    gbug_feature_vuln_path: str\n",
    "    defects_feature_safe_path: str\n",
    "    defects_feature_vuln_path: str\n",
    "    humaneval_feature_safe_path: str\n",
    "    humaneval_feature_vuln_path: str\n",
    "\n",
    "SAE_ACTIVATIONS_GBUG_GPT2 = SAE_ACTIVATIONS(\n",
    "    feature_diff_path=\"../gpt2_gbug-java/layer2/feature_importance_diff.jsonl\",\n",
    "    gbug_feature_safe_path=\"../gpt2_gbug-java/layer2/feature_importance_safe.jsonl\",\n",
    "    gbug_feature_vuln_path=\"../gpt2_gbug-java/layer2/feature_importance_vuln.jsonl\",\n",
    "    defects_feature_safe_path=\"../gpt2_defects4j/layer2/feature_importance_safe.jsonl\",\n",
    "    defects_feature_vuln_path=\"../gpt2_defects4j/layer2/feature_importance_vuln.jsonl\",\n",
    "    humaneval_feature_safe_path=\"../gpt2_humaneval/layer2/feature_importance_safe.jsonl\",\n",
    "    humaneval_feature_vuln_path=\"../gpt2_humaneval/layer2/feature_importance_vuln.jsonl\",\n",
    "    model_path = \"../gpt2_gbug-java/layer2/random_forest_k_25.pt\",\n",
    "    layer = 2,\n",
    "    top_k = 25,\n",
    "    dataset = GBUG_DATASET,\n",
    ")\n",
    "OUR_CONFIG = SAE_ACTIVATIONS_GBUG_GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1fea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [00:31<00:00, 777.73it/s] \n"
     ]
    }
   ],
   "source": [
    "with open(OUR_CONFIG.dataset.training_idx_path, \"r\") as f:\n",
    "    training_indices = json.load(f)\n",
    "\n",
    "train_df_diff = get_diff_data(\n",
    "    SAE_ACTIVATIONS_GBUG_GPT2.feature_diff_path,\n",
    ")\n",
    "top_k = get_most_important_features(train_df_diff, n=OUR_CONFIG.top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd0de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [01:31<00:00, 269.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(OUR_CONFIG.model_path, \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Original (GBUG)\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.gbug_feature_vuln_path,\n",
    "    OUR_CONFIG.gbug_feature_safe_path,\n",
    "    training_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a313a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, test_df):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    X_test = test_df[top_k.tolist()]\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "    y_test = test_df[\"vuln\"]\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tp = sum((y_pred == 1) & (y_test == 1))\n",
    "    fp = sum((y_pred == 1) & (y_test == 0))\n",
    "    tn = sum((y_pred == 0) & (y_test == 0))\n",
    "    fn = sum((y_pred == 0) & (y_test == 1))\n",
    "\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    # Calculate the precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    # Calculate the recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # Calculate the F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed2bd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7,\n",
       " 'precision': 0.6764705882352942,\n",
       " 'recall': 0.7666666666666667,\n",
       " 'f1': 0.71875,\n",
       " 'tp': 23,\n",
       " 'fp': 11,\n",
       " 'fn': 7,\n",
       " 'tn': 19}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_clf(clf, df_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f054dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details:\n",
      "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=2,\n",
      "             param_grid={'max_depth': [None, 50], 'max_features': ['sqrt'],\n",
      "                         'min_samples_leaf': [1, 2],\n",
      "                         'min_samples_split': [2, 5],\n",
      "                         'n_estimators': [100, 300]},\n",
      "             verbose=2)\n",
      "Model parameters:\n",
      "{'cv': 5, 'error_score': nan, 'estimator__bootstrap': True, 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__max_features': 'sqrt', 'estimator__max_leaf_nodes': None, 'estimator__max_samples': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__monotonic_cst': None, 'estimator__n_estimators': 100, 'estimator__n_jobs': None, 'estimator__oob_score': False, 'estimator__random_state': None, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': RandomForestClassifier(), 'n_jobs': 2, 'param_grid': {'n_estimators': [100, 300], 'max_features': ['sqrt'], 'max_depth': [None, 50], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 2}\n"
     ]
    }
   ],
   "source": [
    "# print clf details\n",
    "print(\"Model details:\")\n",
    "print(clf)\n",
    "print(\"Model parameters:\")\n",
    "print(clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 9602/24576 [00:14<00:43, 343.32it/s] "
     ]
    }
   ],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.defects_feature_vuln_path,\n",
    "    OUR_CONFIG.defects_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.humaneval_feature_vuln_path,\n",
    "    OUR_CONFIG.humaneval_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01281b",
   "metadata": {},
   "source": [
    "# Baselines Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaselineClassifier:\n",
    "    path: str\n",
    "    tfidf_vectorizer_path: str\n",
    "    base_dataset: Dataset\n",
    "    input_size: int\n",
    "\n",
    "DEFECTS4J_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/human_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "DEFECTS4J_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/human_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "\n",
    "def load_baseline_classifier(classifier: BaselineClassifier):\n",
    "    with open(classifier.path, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "    return clf\n",
    "@dataclass\n",
    "class Results:\n",
    "    precision: float\n",
    "    recall: float\n",
    "    accuracy: float\n",
    "    f1: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransferabilityPerformance:\n",
    "    on_defects: Optional[int] = None\n",
    "    on_humaneval: Optional[int] = None\n",
    "    on_gbug: Optional[int] = None\n",
    "\n",
    "def calculate_f1_shift(\n",
    "    results: Results,\n",
    "    baseline_results: Results,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the shift in perfoamcene in terms of F1 score % changed\n",
    "    \"\"\"\n",
    "    f1_shift = (results.f1 - baseline_results.f1) / baseline_results.f1 * 100\n",
    "    \n",
    "    return f1_shift\n",
    "\n",
    "def test_baseline_classifier(clf, \n",
    "                             vectorizer, \n",
    "                             df, \n",
    "                             train_indexes,\n",
    "                             before_func_col=\"func_before\", \n",
    "                             after_func_col=\"func_after\") -> Results:\n",
    "    \"\"\"\n",
    "    Test the baseline classifier on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"tokenized_before\"] = df[before_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[after_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    # Pad to 5000 tokens\n",
    "    df[\"tokenized_before\"] = df[\"tokenized_before\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[\"tokenized_after\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "\n",
    "    df_test = df.drop(train_indexes)\n",
    "\n",
    "    df_classical_test = pd.DataFrame()\n",
    "    for row in df_test.iterrows():\n",
    "        row = row[1]\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_before\"].tolist()], \"vuln\": 1}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_after\"].tolist()], \"vuln\": 0}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    X_test = df_classical_test[\"tokens\"].values.tolist()\n",
    "    X_test = [torch.tensor(x) for x in X_test]\n",
    "    y_test = df_classical_test[\"vuln\"]\n",
    "\n",
    "    # Get the prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision, recall, accuracy, f1 = get_metrics(y_pred, y_test)\n",
    "    logger.info(\n",
    "        \"Classification report:\",\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1,\n",
    "    )\n",
    "    return Results(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1,\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_gbug_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_knn_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_knn_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "\n",
    "transf_knn_gbug_on_defects= calculate_f1_shift(\n",
    "    results_gbug_knn_defects,\n",
    "    results_gbug_knn,\n",
    ")\n",
    "transf_knn_gbug_on_human= calculate_f1_shift(\n",
    "    results_gbug_knn_human,\n",
    "    results_gbug_knn,\n",
    ")\n",
    "\n",
    "transf_knn_gbug = TransferabilityPerformance(\n",
    "    on_defects=transf_knn_gbug_on_defects,\n",
    "    on_humaneval=transf_knn_gbug_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest from Gbug\n",
    "results_gbug_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_rf_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_rf_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "transf_rf_gbug_on_defects= calculate_f1_shift(\n",
    "    results_gbug_rf_defects,\n",
    "    results_gbug_rf,\n",
    ")\n",
    "transf_rf_gbug_on_human= calculate_f1_shift(\n",
    "    results_gbug_rf_human,\n",
    "    results_gbug_rf,\n",
    ")\n",
    "\n",
    "transf_rf_gbug = TransferabilityPerformance(\n",
    "    on_defects=transf_rf_gbug_on_defects,\n",
    "    on_humaneval=transf_rf_gbug_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc67a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defects_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_knn_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_knn_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "transf_knn_defects_on_gbug= calculate_f1_shift(\n",
    "    results_defects_knn_gbug,\n",
    "    results_defects_knn,\n",
    ")\n",
    "transf_knn_defects_on_human= calculate_f1_shift(\n",
    "    results_defects_knn_human,\n",
    "    results_defects_knn,\n",
    ")\n",
    "\n",
    "transf_knn_defects = TransferabilityPerformance(\n",
    "    on_gbug=transf_knn_defects_on_gbug,\n",
    "    on_humaneval=transf_knn_defects_on_human,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defects_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_rf_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_rf_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "transf_rf_defects_on_gbug= calculate_f1_shift(\n",
    "    results_defects_rf_gbug,\n",
    "    results_defects_rf,\n",
    ")\n",
    "transf_rf_defects_on_human= calculate_f1_shift(\n",
    "    results_defects_rf_human,\n",
    "    results_defects_rf,\n",
    ")\n",
    "\n",
    "transf_rf_defects = TransferabilityPerformance(\n",
    "    on_gbug=transf_rf_defects_on_gbug,\n",
    "    on_humaneval=transf_rf_defects_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f920934",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_humaneval_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_knn_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_knn_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "transf_knn_humaneval_on_gbug= calculate_f1_shift(\n",
    "    results_humaneval_knn_gbug,\n",
    "    results_humaneval_knn,\n",
    ")\n",
    "transf_knn_humaneval_on_defects= calculate_f1_shift(\n",
    "    results_humaneval_knn_defects,\n",
    "    results_humaneval_knn,\n",
    ")\n",
    "\n",
    "transf_knn_humaneval = TransferabilityPerformance(\n",
    "    on_defects=transf_knn_humaneval_on_defects,\n",
    "    on_gbug=transf_knn_humaneval_on_gbug,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_humaneval_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_rf_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_rf_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "transf_rf_humaneval_on_gbug= calculate_f1_shift(\n",
    "    results_humaneval_rf_gbug,\n",
    "    results_humaneval_rf,\n",
    ")\n",
    "transf_rf_humaneval_on_defects= calculate_f1_shift(\n",
    "    results_humaneval_rf_defects,\n",
    "    results_humaneval_rf,\n",
    ")\n",
    "transf_rf_humaneval = TransferabilityPerformance(\n",
    "    on_defects=transf_rf_humaneval_on_defects,\n",
    "    on_gbug=transf_rf_humaneval_on_gbug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Labels and values\n",
    "labels = [\n",
    "    \"Gbug → Defects4J\", \"Gbug → HumanEval\",\n",
    "    \"Defects4J → Gbug\", \"Defects4J → HumanEval\",\n",
    "    \"HumanEval → Gbug\", \"HumanEval → Defects4J\",\n",
    "]\n",
    "\n",
    "# Split values for KNN and RF\n",
    "values_knn = [\n",
    "    transf_knn_gbug.on_defects, transf_knn_gbug.on_humaneval,\n",
    "    transf_knn_defects.on_gbug, transf_knn_defects.on_humaneval,\n",
    "    transf_knn_humaneval.on_gbug, transf_knn_humaneval.on_defects,\n",
    "]\n",
    "\n",
    "values_rf = [\n",
    "    transf_rf_gbug.on_defects, transf_rf_gbug.on_humaneval,\n",
    "    transf_rf_defects.on_gbug, transf_rf_defects.on_humaneval,\n",
    "    transf_rf_humaneval.on_gbug, transf_rf_humaneval.on_defects,\n",
    "]\n",
    "\n",
    "x = np.arange(len(labels))  # label locations\n",
    "width = 0.4  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "bars1 = ax.barh(x - width/2, values_knn, width, label='KNN', edgecolor='black')\n",
    "bars2 = ax.barh(x + width/2, values_rf, width, label='Random Forest', edgecolor='black')\n",
    "\n",
    "# Aesthetics\n",
    "ax.set_title(\"Transferability Performance of KNN and RF Classifiers\", fontsize=16, pad=15)\n",
    "ax.set_xlabel(\"F1 Shift (%)\", fontsize=12)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlim(-100, 100)\n",
    "ax.axvline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "ax.legend(loc='lower right', frameon=True, fontsize=11)\n",
    "\n",
    "# Annotate values on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.annotate(f'{width:.1f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(5 if width >= 0 else -45, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left' if width >= 0 else 'right',\n",
    "                    va='center',\n",
    "                    fontsize=9,\n",
    "                    color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(\"transferability_performance_pretty.png\", dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
