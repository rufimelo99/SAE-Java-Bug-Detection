{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4540a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c1c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    dataset_path: str\n",
    "    training_idx_path: str\n",
    "\n",
    "GBUG_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/gbug-java.csv\",\n",
    "    training_idx_path=\"../artifacts/gbug-java_train_indexes.json\",\n",
    ")\n",
    "\n",
    "DEFECT_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/defects4j.csv\",\n",
    "    training_idx_path=\"../artifacts/defects4j_train_indexes.json\",\n",
    ")\n",
    "\n",
    "HUMAN_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/humaneval.csv\",\n",
    "    training_idx_path=\"../artifacts/humaneval_train_indexes.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf0c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_testing_dataset(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Get the testing dataset path and the training indexes path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset.dataset_path)\n",
    "    print(df.shape)\n",
    "    with open( dataset.training_idx_path, \"r\") as f:\n",
    "        training_indices = json.load(f)\n",
    "    train_df = df.iloc[training_indices]\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05d736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAE_ACTIVATIONS:\n",
    "    feature_diff_path: str\n",
    "    model_path: str\n",
    "    layer: int\n",
    "    top_k: int\n",
    "    dataset: Dataset\n",
    "    gbug_feature_safe_path: str\n",
    "    gbug_feature_vuln_path: str\n",
    "    defects_feature_safe_path: str\n",
    "    defects_feature_vuln_path: str\n",
    "    humaneval_feature_safe_path: str\n",
    "    humaneval_feature_vuln_path: str\n",
    "\n",
    "SAE_ACTIVATIONS_GBUG_LAYER_1 = SAE_ACTIVATIONS(\n",
    "    feature_diff_path=\"../gpt2_gbug-java/layer1/feature_importance_diff.jsonl\",\n",
    "    gbug_feature_safe_path=\"../gpt2_gbug-java/layer1/feature_importance_safe.jsonl\",\n",
    "    gbug_feature_vuln_path=\"../gpt2_gbug-java/layer1/feature_importance_vuln.jsonl\",\n",
    "    defects_feature_safe_path=\"../gpt2_defects4j/layer1/feature_importance_safe.jsonl\",\n",
    "    defects_feature_vuln_path=\"../gpt2_defects4j/layer1/feature_importance_vuln.jsonl\",\n",
    "    humaneval_feature_safe_path=\"../gpt2_humaneval/layer1/feature_importance_safe.jsonl\",\n",
    "    humaneval_feature_vuln_path=\"../gpt2_humaneval/layer1/feature_importance_vuln.jsonl\",\n",
    "    model_path = \"models/gbug_decision_tree_layer10_k_76.pt\",\n",
    "    layer = 10,\n",
    "    top_k = 76,\n",
    "    dataset = GBUG_DATASET,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5df5d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_jsonl_file(jsonl_path):\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def get_feature_diff_path(dataset: Dataset, training_indexes):\n",
    "    \"\"\"\n",
    "    Get the feature diff path for the given dataset.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    diff_data = list(read_jsonl_file(dataset.feature_diff_path))\n",
    "    diff_df = pd.DataFrame(diff_data)\n",
    "\n",
    "    columns = diff_df.columns.to_list()\n",
    "    columns.remove(\"values\")\n",
    "\n",
    "    diff_df.drop(columns=columns, inplace=True)\n",
    "\n",
    "    for i in trange(len(diff_df[\"values\"][0])):\n",
    "        diff_df[f\"feature_{i}\"] = diff_df[\"values\"].apply(lambda x: x[i])\n",
    "\n",
    "    diff_df.drop(columns=[\"values\"], inplace=True)\n",
    "\n",
    "    \n",
    "    return diff_df\n",
    "\n",
    "\n",
    "\n",
    "def get_most_important_features(train_df_diff, n=100):\n",
    "    features = train_df_diff.sum(axis=0).sort_values(ascending=False).index[1 : n + 1]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d900c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUR_CONFIG = SAE_ACTIVATIONS_GBUG_LAYER_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd1fea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [00:33<00:00, 739.03it/s]\n"
     ]
    }
   ],
   "source": [
    "_, test_df = get_testing_dataset(OUR_CONFIG.dataset)\n",
    "\n",
    "with open(OUR_CONFIG.dataset.training_idx_path, \"r\") as f:\n",
    "    training_indices = json.load(f)\n",
    "\n",
    "train_df_diff = get_feature_diff_path(\n",
    "    SAE_ACTIVATIONS_GBUG_LAYER_1, training_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7cb728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = get_most_important_features(train_df_diff, n=OUR_CONFIG.top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e4bdf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vuln_safe_data(vuln_jsonl_path, safe_jsonl_path, train_indexes):\n",
    "    vuln_data = list(read_jsonl_file(vuln_jsonl_path))\n",
    "    safe_data = list(read_jsonl_file(safe_jsonl_path))\n",
    "    vuln_df = pd.DataFrame(vuln_data)\n",
    "    safe_df = pd.DataFrame(safe_data)\n",
    "    vuln_df.drop(columns=[\"labels\", \"model\", \"plot_type\"], inplace=True)\n",
    "    vuln_df[\"vuln\"] = 1\n",
    "\n",
    "    safe_df.drop(columns=[\"labels\", \"model\", \"plot_type\"], inplace=True)\n",
    "    safe_df[\"vuln\"] = 0\n",
    "\n",
    "    for i in trange(len(vuln_df[\"values\"][0])):\n",
    "        vuln_df[f\"feature_{i}\"] = vuln_df[\"values\"].apply(lambda x: x[i])\n",
    "        safe_df[f\"feature_{i}\"] = safe_df[\"values\"].apply(lambda x: x[i])\n",
    "\n",
    "    safe_df_train = safe_df.loc[train_indexes]\n",
    "    safe_df_test = safe_df.drop(train_indexes)\n",
    "\n",
    "    vuln_df_train = vuln_df.loc[train_indexes]\n",
    "    vuln_df_test = vuln_df.drop(train_indexes)\n",
    "\n",
    "    df_train = pd.concat([safe_df_train, vuln_df_train])\n",
    "    df_test = pd.concat([safe_df_test, vuln_df_test])\n",
    "\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "    df_train.drop(columns=[\"values\"], inplace=True)\n",
    "    df_test.drop(columns=[\"values\"], inplace=True)\n",
    "\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a313a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, test_df):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \"\"\"\n",
    "    #bug_id\tfunc_before\tfunc_after\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i, row in tqdm(test_df.iterrows()):\n",
    "        X = row[top_k.tolist()]\n",
    "        y = row[\"vuln\"]\n",
    "\n",
    "        # Get the prediction\n",
    "        pred = clf.predict([X])\n",
    "\n",
    "        # Update the confusion matrix\n",
    "        if pred == 1 and y == 1:\n",
    "            tp += 1\n",
    "        elif pred == 1 and y == 0:\n",
    "            fp += 1\n",
    "        elif pred == 0 and y == 1:\n",
    "            fn += 1\n",
    "        elif pred == 0 and y == 0:\n",
    "            tn += 1\n",
    "\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    # Calculate the precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    # Calculate the recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # Calculate the F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dd0de61",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/gbug_decision_tree_layer10_k_76.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSAE_ACTIVATIONS_GBUG_LAYER_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     clf \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Original (GBUG)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/gbug_decision_tree_layer10_k_76.pt'"
     ]
    }
   ],
   "source": [
    "with open(SAE_ACTIVATIONS_GBUG_LAYER_1.model_path, \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Original (GBUG)\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.gbug_feature_vuln_path,\n",
    "    OUR_CONFIG.gbug_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [01:47<00:00, 227.99it/s]\n",
      "698it [00:00, 5400.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5286532951289399,\n",
       " 'precision': 0.5208333333333334,\n",
       " 'recall': 0.7163323782234957,\n",
       " 'f1': 0.6031363088057902,\n",
       " 'tp': 250,\n",
       " 'fp': 230,\n",
       " 'fn': 99,\n",
       " 'tn': 119}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.defects_feature_vuln_path,\n",
    "    OUR_CONFIG.defects_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f06f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [01:38<00:00, 249.19it/s]\n",
      "88it [00:00, 4920.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5,\n",
       " 'precision': 0.5,\n",
       " 'recall': 0.8409090909090909,\n",
       " 'f1': 0.6271186440677967,\n",
       " 'tp': 37,\n",
       " 'fp': 37,\n",
       " 'fn': 7,\n",
       " 'tn': 7}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.humaneval_feature_vuln_path,\n",
    "    OUR_CONFIG.humaneval_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01281b",
   "metadata": {},
   "source": [
    "# Baselines Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaselineClassifier:\n",
    "    path: str\n",
    "    tfidf_vectorizer_path: str\n",
    "    base_dataset: Dataset\n",
    "    input_size: int\n",
    "\n",
    "DEFECTS4J_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/humaneval_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "DEFECTS4J_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/humaneval_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drl_patches.sparse_autoencoders.get_vectorizer import load_tfidf_vectorizer\n",
    "from drl_patches.sparse_autoencoders.getting_experiment_config import (\n",
    "    load_training_indexes,\n",
    ")\n",
    "import torch\n",
    "from drl_patches.logger import logger\n",
    "from drl_patches.sparse_autoencoders.classical_data_mining import get_metrics\n",
    "\n",
    "def load_baseline_classifier(classifier: BaselineClassifier):\n",
    "    with open(classifier.path, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 4632.69it/s]\n",
      "100%|██████████| 148/148 [00:00<00:00, 5258.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-21 12:16:54\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mClassification report:        \u001b[0m \u001b[36maccuracy\u001b[0m=\u001b[35m0.6666666666666666\u001b[0m \u001b[36mf1\u001b[0m=\u001b[35m0.6774193548387096\u001b[0m \u001b[36mprecision\u001b[0m=\u001b[35m0.65625\u001b[0m \u001b[36mrecall\u001b[0m=\u001b[35m0.7\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 465/465 [00:00<00:00, 6852.07it/s]\n",
      "100%|██████████| 465/465 [00:00<00:00, 6867.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-21 12:16:54\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mClassification report:        \u001b[0m \u001b[36maccuracy\u001b[0m=\u001b[35m0.5\u001b[0m \u001b[36mf1\u001b[0m=\u001b[35m0.4431137724550898\u001b[0m \u001b[36mprecision\u001b[0m=\u001b[35m0.5\u001b[0m \u001b[36mrecall\u001b[0m=\u001b[35m0.3978494623655914\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:00<00:00, 7263.25it/s]\n",
      "100%|██████████| 162/162 [00:00<00:00, 7650.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-21 12:16:54\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mClassification report:        \u001b[0m \u001b[36maccuracy\u001b[0m=\u001b[35m0.48484848484848486\u001b[0m \u001b[36mf1\u001b[0m=\u001b[35m0.43333333333333324\u001b[0m \u001b[36mprecision\u001b[0m=\u001b[35m0.48148148148148145\u001b[0m \u001b[36mrecall\u001b[0m=\u001b[35m0.3939393939393939\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_baseline_classifier(clf, \n",
    "                             vectorizer, \n",
    "                             df, \n",
    "                             train_indexes,\n",
    "                             before_func_col=\"func_before\", \n",
    "                             after_func_col=\"func_after\"):\n",
    "    \"\"\"\n",
    "    Test the baseline classifier on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"tokenized_before\"] = df[before_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[after_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    # Pad to 5000 tokens\n",
    "    df[\"tokenized_before\"] = df[\"tokenized_before\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[\"tokenized_after\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "\n",
    "    df_test = df.drop(train_indexes)\n",
    "\n",
    "    df_classical_test = pd.DataFrame()\n",
    "    for row in df_test.iterrows():\n",
    "        row = row[1]\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_before\"].tolist()], \"vuln\": 1}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_after\"].tolist()], \"vuln\": 0}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    X_test = df_classical_test[\"tokens\"].values.tolist()\n",
    "    X_test = [torch.tensor(x) for x in X_test]\n",
    "    y_test = df_classical_test[\"vuln\"]\n",
    "\n",
    "    # Get the prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision, recall, accuracy, f1 = get_metrics(y_pred, y_test)\n",
    "    logger.info(\n",
    "        \"Classification report:\",\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "results = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c1822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
