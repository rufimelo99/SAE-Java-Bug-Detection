{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from drl_patches.sparse_autoencoders.get_vectorizer import load_tfidf_vectorizer\n",
    "from drl_patches.sparse_autoencoders.getting_experiment_config import (\n",
    "    load_training_indexes,\n",
    ")\n",
    "import torch\n",
    "from drl_patches.logger import logger\n",
    "from drl_patches.sparse_autoencoders.classical_data_mining import get_metrics\n",
    "from drl_patches.sparse_autoencoders.utils import read_jsonl_file, set_seed\n",
    "from drl_patches.sparse_autoencoders.vulnerability_detection_features import (\n",
    "    get_diff_data, get_most_important_features, get_vuln_safe_data\n",
    ")\n",
    "set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    dataset_path: str\n",
    "    training_idx_path: str\n",
    "\n",
    "GBUG_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/gbug-java.csv\",\n",
    "    training_idx_path=\"../artifacts/gbug-java_train_indexes.json\",\n",
    ")\n",
    "\n",
    "DEFECT_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/defects4j.csv\",\n",
    "    training_idx_path=\"../artifacts/defects4j_train_indexes.json\",\n",
    ")\n",
    "\n",
    "HUMAN_DATASET = Dataset(\n",
    "    dataset_path=\"../artifacts/humaneval.csv\",\n",
    "    training_idx_path=\"../artifacts/humaneval_train_indexes.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_testing_dataset(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Get the testing dataset path and the training indexes path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset.dataset_path)\n",
    "    with open( dataset.training_idx_path, \"r\") as f:\n",
    "        training_indices = json.load(f)\n",
    "    train_df = df.loc[training_indices]\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAE_ACTIVATIONS:\n",
    "    feature_diff_path: str\n",
    "    model_path: str\n",
    "    layer: int\n",
    "    top_k: int\n",
    "    dataset: Dataset\n",
    "    gbug_feature_safe_path: str\n",
    "    gbug_feature_vuln_path: str\n",
    "    defects_feature_safe_path: str\n",
    "    defects_feature_vuln_path: str\n",
    "    humaneval_feature_safe_path: str\n",
    "    humaneval_feature_vuln_path: str\n",
    "\n",
    "SAE_ACTIVATIONS_GBUG_GPT2 = SAE_ACTIVATIONS(\n",
    "    feature_diff_path=\"../gpt2_gbug-java/layer2/feature_importance_diff.jsonl\",\n",
    "    gbug_feature_safe_path=\"../gpt2_gbug-java/layer2/feature_importance_safe.jsonl\",\n",
    "    gbug_feature_vuln_path=\"../gpt2_gbug-java/layer2/feature_importance_vuln.jsonl\",\n",
    "    defects_feature_safe_path=\"../gpt2_defects4j/layer2/feature_importance_safe.jsonl\",\n",
    "    defects_feature_vuln_path=\"../gpt2_defects4j/layer2/feature_importance_vuln.jsonl\",\n",
    "    humaneval_feature_safe_path=\"../gpt2_humaneval/layer2/feature_importance_safe.jsonl\",\n",
    "    humaneval_feature_vuln_path=\"../gpt2_humaneval/layer2/feature_importance_vuln.jsonl\",\n",
    "    model_path = \"../gpt2_gbug-java/layer2/random_forest_k_25.pt\",\n",
    "    layer = 2,\n",
    "    top_k = 25,\n",
    "    dataset = GBUG_DATASET,\n",
    ")\n",
    "OUR_CONFIG = SAE_ACTIVATIONS_GBUG_GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUR_CONFIG.dataset.training_idx_path, \"r\") as f:\n",
    "    training_indices = json.load(f)\n",
    "\n",
    "train_df_diff = get_diff_data(\n",
    "    SAE_ACTIVATIONS_GBUG_GPT2.feature_diff_path,\n",
    ")\n",
    "top_k = get_most_important_features(train_df_diff, n=OUR_CONFIG.top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed4c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = ['feature_11081', 'feature_21514', 'feature_2624', 'feature_5277',\n",
    "       'feature_7675', 'feature_264', 'feature_352', 'feature_757',\n",
    "       'feature_14423', 'feature_23548', 'feature_21042', 'feature_22713',\n",
    "       'feature_13626', 'feature_6039', 'feature_23015', 'feature_11145',\n",
    "       'feature_13330', 'feature_3889', 'feature_12711', 'feature_2264',\n",
    "       'feature_21654', 'feature_5238', 'feature_20133', 'feature_22422',\n",
    "       'feature_262']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dd0de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24576/24576 [01:25<00:00, 286.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(OUR_CONFIG.model_path, \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Original (GBUG)\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.gbug_feature_vuln_path,\n",
    "    OUR_CONFIG.gbug_feature_safe_path,\n",
    "    training_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, test_df):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    X_test = test_df[top_k.tolist()]\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "    y_test = test_df[\"vuln\"]\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tp = sum((y_pred == 1) & (y_test == 1))\n",
    "    fp = sum((y_pred == 1) & (y_test == 0))\n",
    "    tn = sum((y_pred == 0) & (y_test == 0))\n",
    "    fn = sum((y_pred == 0) & (y_test == 1))\n",
    "\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    # Calculate the precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    # Calculate the recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # Calculate the F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed2bd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'precision': 0.8,\n",
       " 'recall': 0.8,\n",
       " 'f1': 0.8000000000000002,\n",
       " 'tp': 24,\n",
       " 'fp': 6,\n",
       " 'fn': 6,\n",
       " 'tn': 24}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_clf(clf, df_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f054dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print clf details\n",
    "print(\"Model details:\")\n",
    "print(clf)\n",
    "print(\"Model parameters:\")\n",
    "print(clf.get_params())\n",
    "print(\"Best parameters:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.defects_feature_vuln_path,\n",
    "    OUR_CONFIG.defects_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defects4J\n",
    "_, df_test = get_vuln_safe_data(\n",
    "    OUR_CONFIG.humaneval_feature_vuln_path,\n",
    "    OUR_CONFIG.humaneval_feature_safe_path,\n",
    "    training_indices,\n",
    ")\n",
    "# filter columns based on top_k\n",
    "df_test_filtered = df_test[\n",
    "    top_k.tolist() + [\"vuln\"]\n",
    "]\n",
    "\n",
    "results = evaluate_clf(clf, df_test_filtered)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01281b",
   "metadata": {},
   "source": [
    "# Baselines Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaselineClassifier:\n",
    "    path: str\n",
    "    tfidf_vectorizer_path: str\n",
    "    base_dataset: Dataset\n",
    "    input_size: int\n",
    "\n",
    "DEFECTS4J_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_KNN_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/human_knn_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "DEFECTS4J_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/defects4j_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=DEFECT_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "GBUG_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/gbug_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=GBUG_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "HUMANEVAL_RF_BASELINE = BaselineClassifier(\n",
    "    path=\"../ole/human_random_forest_k_5000.pt\",\n",
    "    tfidf_vectorizer_path = \"../artifacts/vectorizer.pkl\",\n",
    "    base_dataset=HUMAN_DATASET,\n",
    "    input_size=5000,\n",
    ")\n",
    "\n",
    "def load_baseline_classifier(classifier: BaselineClassifier):\n",
    "    with open(classifier.path, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "    return clf\n",
    "@dataclass\n",
    "class Results:\n",
    "    precision: float\n",
    "    recall: float\n",
    "    accuracy: float\n",
    "    f1: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransferabilityPerformance:\n",
    "    on_defects: Optional[int] = None\n",
    "    on_humaneval: Optional[int] = None\n",
    "    on_gbug: Optional[int] = None\n",
    "\n",
    "def calculate_f1_shift(\n",
    "    results: Results,\n",
    "    baseline_results: Results,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the shift in perfoamcene in terms of F1 score % changed\n",
    "    \"\"\"\n",
    "    f1_shift = (results.f1 - baseline_results.f1) / baseline_results.f1 * 100\n",
    "    \n",
    "    return f1_shift\n",
    "\n",
    "def test_baseline_classifier(clf, \n",
    "                             vectorizer, \n",
    "                             df, \n",
    "                             train_indexes,\n",
    "                             before_func_col=\"func_before\", \n",
    "                             after_func_col=\"func_after\") -> Results:\n",
    "    \"\"\"\n",
    "    Test the baseline classifier on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"tokenized_before\"] = df[before_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[after_func_col].progress_apply(\n",
    "        lambda x: vectorizer.transform([x]).toarray()[0]\n",
    "    )\n",
    "    # Pad to 5000 tokens\n",
    "    df[\"tokenized_before\"] = df[\"tokenized_before\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "    df[\"tokenized_after\"] = df[\"tokenized_after\"].apply(\n",
    "        lambda x: x[:5000] + [0] * (5000 - len(x)) if len(x) < 5000 else x[:5000]\n",
    "    )\n",
    "\n",
    "    df_test = df.drop(train_indexes)\n",
    "\n",
    "    df_classical_test = pd.DataFrame()\n",
    "    for row in df_test.iterrows():\n",
    "        row = row[1]\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_before\"].tolist()], \"vuln\": 1}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        df_classical_test = pd.concat(\n",
    "            [\n",
    "                df_classical_test,\n",
    "                pd.DataFrame(\n",
    "                    {\"tokens\": [row[\"tokenized_after\"].tolist()], \"vuln\": 0}, index=[0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    X_test = df_classical_test[\"tokens\"].values.tolist()\n",
    "    X_test = [torch.tensor(x) for x in X_test]\n",
    "    y_test = df_classical_test[\"vuln\"]\n",
    "\n",
    "    # Get the prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision, recall, accuracy, f1 = get_metrics(y_pred, y_test)\n",
    "    logger.info(\n",
    "        \"Classification report:\",\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1,\n",
    "    )\n",
    "    return Results(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1,\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_gbug_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_knn_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_knn_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "\n",
    "transf_knn_gbug_on_defects= calculate_f1_shift(\n",
    "    results_gbug_knn_defects,\n",
    "    results_gbug_knn,\n",
    ")\n",
    "transf_knn_gbug_on_human= calculate_f1_shift(\n",
    "    results_gbug_knn_human,\n",
    "    results_gbug_knn,\n",
    ")\n",
    "\n",
    "transf_knn_gbug = TransferabilityPerformance(\n",
    "    on_defects=transf_knn_gbug_on_defects,\n",
    "    on_humaneval=transf_knn_gbug_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest from Gbug\n",
    "results_gbug_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_rf_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "results_gbug_rf_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(GBUG_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(GBUG_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "\n",
    "transf_rf_gbug_on_defects= calculate_f1_shift(\n",
    "    results_gbug_rf_defects,\n",
    "    results_gbug_rf,\n",
    ")\n",
    "transf_rf_gbug_on_human= calculate_f1_shift(\n",
    "    results_gbug_rf_human,\n",
    "    results_gbug_rf,\n",
    ")\n",
    "\n",
    "transf_rf_gbug = TransferabilityPerformance(\n",
    "    on_defects=transf_rf_gbug_on_defects,\n",
    "    on_humaneval=transf_rf_gbug_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc67a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defects_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_knn_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_knn_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "transf_knn_defects_on_gbug= calculate_f1_shift(\n",
    "    results_defects_knn_gbug,\n",
    "    results_defects_knn,\n",
    ")\n",
    "transf_knn_defects_on_human= calculate_f1_shift(\n",
    "    results_defects_knn_human,\n",
    "    results_defects_knn,\n",
    ")\n",
    "\n",
    "transf_knn_defects = TransferabilityPerformance(\n",
    "    on_gbug=transf_knn_defects_on_gbug,\n",
    "    on_humaneval=transf_knn_defects_on_human,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_defects_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_rf_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_defects_rf_human = test_baseline_classifier(\n",
    "    load_baseline_classifier(DEFECTS4J_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(DEFECTS4J_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "transf_rf_defects_on_gbug= calculate_f1_shift(\n",
    "    results_defects_rf_gbug,\n",
    "    results_defects_rf,\n",
    ")\n",
    "transf_rf_defects_on_human= calculate_f1_shift(\n",
    "    results_defects_rf_human,\n",
    "    results_defects_rf,\n",
    ")\n",
    "\n",
    "transf_rf_defects = TransferabilityPerformance(\n",
    "    on_gbug=transf_rf_defects_on_gbug,\n",
    "    on_humaneval=transf_rf_defects_on_human,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f920934",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_humaneval_knn = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_knn_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_knn_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_KNN_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_KNN_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "transf_knn_humaneval_on_gbug= calculate_f1_shift(\n",
    "    results_humaneval_knn_gbug,\n",
    "    results_humaneval_knn,\n",
    ")\n",
    "transf_knn_humaneval_on_defects= calculate_f1_shift(\n",
    "    results_humaneval_knn_defects,\n",
    "    results_humaneval_knn,\n",
    ")\n",
    "\n",
    "transf_knn_humaneval = TransferabilityPerformance(\n",
    "    on_defects=transf_knn_humaneval_on_defects,\n",
    "    on_gbug=transf_knn_humaneval_on_gbug,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_humaneval_rf = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(HUMAN_DATASET.dataset_path),\n",
    "    load_training_indexes(HUMAN_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_rf_gbug = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(GBUG_DATASET.dataset_path),\n",
    "    load_training_indexes(GBUG_DATASET.training_idx_path),\n",
    ")\n",
    "results_humaneval_rf_defects = test_baseline_classifier(\n",
    "    load_baseline_classifier(HUMANEVAL_RF_BASELINE),\n",
    "    load_tfidf_vectorizer(HUMANEVAL_RF_BASELINE.tfidf_vectorizer_path),\n",
    "    pd.read_csv(DEFECT_DATASET.dataset_path),\n",
    "    load_training_indexes(DEFECT_DATASET.training_idx_path),\n",
    ")\n",
    "transf_rf_humaneval_on_gbug= calculate_f1_shift(\n",
    "    results_humaneval_rf_gbug,\n",
    "    results_humaneval_rf,\n",
    ")\n",
    "transf_rf_humaneval_on_defects= calculate_f1_shift(\n",
    "    results_humaneval_rf_defects,\n",
    "    results_humaneval_rf,\n",
    ")\n",
    "transf_rf_humaneval = TransferabilityPerformance(\n",
    "    on_defects=transf_rf_humaneval_on_defects,\n",
    "    on_gbug=transf_rf_humaneval_on_gbug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Labels and values\n",
    "labels = [\n",
    "    \"Gbug → Defects4J\", \"Gbug → HumanEval\",\n",
    "    \"Defects4J → Gbug\", \"Defects4J → HumanEval\",\n",
    "    \"HumanEval → Gbug\", \"HumanEval → Defects4J\",\n",
    "]\n",
    "\n",
    "# Split values for KNN and RF\n",
    "values_knn = [\n",
    "    transf_knn_gbug.on_defects, transf_knn_gbug.on_humaneval,\n",
    "    transf_knn_defects.on_gbug, transf_knn_defects.on_humaneval,\n",
    "    transf_knn_humaneval.on_gbug, transf_knn_humaneval.on_defects,\n",
    "]\n",
    "\n",
    "values_rf = [\n",
    "    transf_rf_gbug.on_defects, transf_rf_gbug.on_humaneval,\n",
    "    transf_rf_defects.on_gbug, transf_rf_defects.on_humaneval,\n",
    "    transf_rf_humaneval.on_gbug, transf_rf_humaneval.on_defects,\n",
    "]\n",
    "\n",
    "x = np.arange(len(labels))  # label locations\n",
    "width = 0.4  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "bars1 = ax.barh(x - width/2, values_knn, width, label='KNN', edgecolor='black')\n",
    "bars2 = ax.barh(x + width/2, values_rf, width, label='Random Forest', edgecolor='black')\n",
    "\n",
    "# Aesthetics\n",
    "ax.set_title(\"Transferability Performance of KNN and RF Classifiers\", fontsize=16, pad=15)\n",
    "ax.set_xlabel(\"F1 Shift (%)\", fontsize=12)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlim(-100, 100)\n",
    "ax.axvline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "ax.legend(loc='lower right', frameon=True, fontsize=11)\n",
    "\n",
    "# Annotate values on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.annotate(f'{width:.1f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(5 if width >= 0 else -45, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left' if width >= 0 else 'right',\n",
    "                    va='center',\n",
    "                    fontsize=9,\n",
    "                    color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(\"transferability_performance_pretty.png\", dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
