import json
import os
import random
import warnings

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm, trange

warnings.filterwarnings("ignore")
import argparse
from enum import Enum
from typing import List

from drl_patches.logger import logger
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier


class ClassifierType(str, Enum):
    LOGISTIC_REGRESSION = "logistic_regression"
    SVM = "svm"
    KNN = "knn"
    DECISION_TREE = "decision_tree"
    RANDOM_FOREST = "random_forest"


sk_classifiers_map = {
    ClassifierType.LOGISTIC_REGRESSION: LogisticRegression(),
    ClassifierType.SVM: svm.SVC(),
    ClassifierType.KNN: KNeighborsClassifier(),
    ClassifierType.DECISION_TREE: DecisionTreeClassifier(),
    ClassifierType.RANDOM_FOREST: RandomForestClassifier(),
}

parameters_map = {
    ClassifierType.LOGISTIC_REGRESSION: {
        "C": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
        "max_iter": [1000],
        "solver": ["newton-cg", "lbfgs"],
    },
    ClassifierType.SVM: {
        "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],
        "kernel": ["linear", "poly", "rbf", "sigmoid"],
    },
    ClassifierType.KNN: {
        "n_neighbors": [1, 2, 3, 5, 10, 100, 1000],
        "weights": ["distance"],
        "metric": ["euclidean"],
    },
    ClassifierType.DECISION_TREE: {
        "criterion": ["gini", "entropy"],
        "splitter": ["best", "random"],
    },
    ClassifierType.RANDOM_FOREST: {
        "n_estimators": [10, 100, 1000],
        "max_features": ["sqrt", "log2"],
        "max_depth": [1000],
        "min_samples_split": [2, 10, 100],
        "min_samples_leaf": [1, 10, 100],
    },
}


# Set up seeds
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
random.seed(seed)


def read_jsonl_file(jsonl_path):
    with open(jsonl_path, "r") as f:
        for line in f:
            yield json.loads(line)


def get_diff_data(diff_jsonl_path):
    diff_data = list(read_jsonl_file(diff_jsonl_path))
    diff_df = pd.DataFrame(diff_data)

    columns = diff_df.columns.to_list()
    columns.remove("values")

    diff_df.drop(columns=columns, inplace=True)

    for i in trange(len(diff_df["values"][0])):
        diff_df[f"feature_{i}"] = diff_df["values"].apply(lambda x: x[i])

    diff_df.drop(columns=["values"], inplace=True)

    return diff_df


def get_training_indexes(diff_df):
    return np.random.choice(diff_df.index, int(len(diff_df) * 0.8), replace=False)


def get_vuln_safe_data(vuln_jsonl_path, safe_jsonl_path):
    vuln_data = list(read_jsonl_file(vuln_jsonl_path))
    safe_data = list(read_jsonl_file(safe_jsonl_path))
    vuln_df = pd.DataFrame(vuln_data)
    safe_df = pd.DataFrame(safe_data)
    vuln_df.drop(columns=["labels", "model", "plot_type"], inplace=True)
    vuln_df["vuln"] = 1

    safe_df.drop(columns=["labels", "model", "plot_type"], inplace=True)
    safe_df["vuln"] = 0

    for i in trange(len(vuln_df["values"][0])):
        vuln_df[f"feature_{i}"] = vuln_df["values"].apply(lambda x: x[i])
        safe_df[f"feature_{i}"] = safe_df["values"].apply(lambda x: x[i])

    safe_df_train = safe_df.loc[train_indexes]
    safe_df_test = safe_df.drop(train_indexes)

    vuln_df_train = vuln_df.loc[train_indexes]
    vuln_df_test = vuln_df.drop(train_indexes)

    df_train = pd.concat([safe_df_train, vuln_df_train])
    df_test = pd.concat([safe_df_test, vuln_df_test])

    df_train = df_train.sample(frac=1).reset_index(drop=True)
    df_test = df_test.sample(frac=1).reset_index(drop=True)
    df_train.drop(columns=["values"], inplace=True)
    df_test.drop(columns=["values"], inplace=True)

    return df_train, df_test


def get_most_important_features(train_df_diff, n=100):
    return train_df_diff.sum(axis=0).sort_values(ascending=False).index[1 : n + 1]


def store_classifier_info(
    jsonl_path, classifier: ClassifierType, n_features, tp, fp, tn, fn, y_pred, y_test
):
    with open(jsonl_path, "a") as f:
        f.write(
            json.dumps(
                {
                    "classifier": classifier.value,
                    "n_features": n_features,
                    "tp": tp,
                    "fp": fp,
                    "tn": tn,
                    "fn": fn,
                    "y_pred": y_pred.tolist(),
                    "y_test": y_test.tolist(),
                }
            )
            + "\n"
        )


def train_model(
    df_train,
    df_test,
    top_k_features,
    sk_classifiers: List[ClassifierType],
    directory="artifacts",
):

    most_important_cols = get_most_important_features(train_df_diff, n=top_k_features)

    X_train = df_train[most_important_cols]
    y_train = df_train["vuln"]

    X_test = df_test[most_important_cols]
    y_test = df_test["vuln"]

    for clf in sk_classifiers:
        classifier = sk_classifiers_map[clf]
        classifier = GridSearchCV(classifier, param_grid=parameters_map[clf], cv=5, verbose=3)

        classifier.fit(X_train, y_train)
        y_pred = classifier.predict(X_test)
        tp = sum((y_pred == 1) & (y_test == 1))
        fp = sum((y_pred == 1) & (y_test == 0))
        tn = sum((y_pred == 0) & (y_test == 0))
        fn = sum((y_pred == 0) & (y_test == 1))
        store_classifier_info(
            directory + f"/{clf.value}.jsonl",
            clf,
            top_k_features,
            tp,
            fp,
            tn,
            fn,
            y_pred,
            y_test,
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--dir-path",
        type=str,
        default="artifacts",
    )

    args = parser.parse_args()

    logger.info("Reading data.")
    diff_df = get_diff_data(
        os.path.join(args.dir_path, "feature_importance_diff.jsonl")
    )
    train_indexes = get_training_indexes(diff_df)
    train_df_diff = diff_df.loc[train_indexes]

    df_train, df_test = get_vuln_safe_data(
        os.path.join(args.dir_path, "feature_importance_vuln.jsonl"),
        os.path.join(args.dir_path, "feature_importance_safe.jsonl"),
    )

    logger.info("Training models.")
    top_k_features = [1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]
    for k in tqdm(top_k_features):
        train_model(
            df_train,
            df_test,
            k,
            [
                ClassifierType.LOGISTIC_REGRESSION,
                # ClassifierType.SVM,
                # ClassifierType.KNN,
                ClassifierType.DECISION_TREE,
            ],
            args.dir_path,
        )
