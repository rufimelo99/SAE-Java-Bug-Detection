import json
import os
import random
import warnings

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm, trange

warnings.filterwarnings("ignore")
import argparse
from enum import Enum
from typing import List

from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

from drl_patches.logger import logger


class ClassifierType(str, Enum):
    LOGISTIC_REGRESSION = "logistic_regression"
    SVM = "svm"
    KNN = "knn"
    DECISION_TREE = "decision_tree"


sk_classifiers_map = {
    ClassifierType.LOGISTIC_REGRESSION: LogisticRegression(),
    ClassifierType.SVM: svm.SVC(),
    ClassifierType.KNN: KNeighborsClassifier(),
    ClassifierType.DECISION_TREE: DecisionTreeClassifier(),
}

# Set up seeds
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
random.seed(seed)


def read_jsonl_file(jsonl_path):
    with open(jsonl_path, "r") as f:
        for line in f:
            yield json.loads(line)


def get_diff_data(diff_jsonl_path):
    diff_data = list(read_jsonl_file(diff_jsonl_path))
    diff_df = pd.DataFrame(diff_data)

    for i in trange(len(diff_df["logit_diff"][0])):
        diff_df[f"feature_{i}"] = diff_df["logit_diff"].apply(lambda x: x[i])

    diff_df.drop(columns=["logit_diff", "labels", "model", "plot_type"], inplace=True)

    return diff_df


def get_training_indexes(diff_df):
    return np.random.choice(diff_df.index, int(len(diff_df) * 0.8), replace=False)


def get_vuln_safe_data(vuln_jsonl_path, safe_jsonl_path):
    vuln_data = list(read_jsonl_file(vuln_jsonl_path))
    safe_data = list(read_jsonl_file(safe_jsonl_path))
    vuln_df = pd.DataFrame(vuln_data)
    safe_df = pd.DataFrame(safe_data)
    vuln_df.drop(columns=["labels", "model", "plot_type"], inplace=True)
    vuln_df["vuln"] = 1

    safe_df.drop(columns=["labels", "model", "plot_type"], inplace=True)
    safe_df["vuln"] = 0

    for i in trange(len(vuln_df["logit_diff"][0])):
        vuln_df[f"feature_{i}"] = vuln_df["logit_diff"].apply(lambda x: x[i])
        safe_df[f"feature_{i}"] = safe_df["logit_diff"].apply(lambda x: x[i])

    safe_df_train = safe_df.loc[train_indexes]
    safe_df_test = safe_df.drop(train_indexes)

    vuln_df_train = vuln_df.loc[train_indexes]
    vuln_df_test = vuln_df.drop(train_indexes)

    df_train = pd.concat([safe_df_train, vuln_df_train])
    df_test = pd.concat([safe_df_test, vuln_df_test])

    df_train = df_train.sample(frac=1).reset_index(drop=True)
    df_test = df_test.sample(frac=1).reset_index(drop=True)
    df_train.drop(columns=["logit_diff"], inplace=True)
    df_test.drop(columns=["logit_diff"], inplace=True)

    return df_train, df_test


def get_most_important_features(train_df_diff, n=100):
    return train_df_diff.sum(axis=0).sort_values(ascending=False).index[1 : n + 1]


def store_classifier_info(
    jsonl_path, classifier: ClassifierType, n_features, tp, fp, tn, fn, y_pred, y_test
):
    with open(jsonl_path, "a") as f:
        f.write(
            json.dumps(
                {
                    "classifier": classifier.value,
                    "n_features": n_features.item(),
                    "tp": tp,
                    "fp": fp,
                    "tn": tn,
                    "fn": fn,
                    "y_pred": y_pred.tolist(),
                    "y_test": y_test.tolist(),
                }
            )
            + "\n"
        )


def train_model(
    df_train,
    df_test,
    top_k_features,
    sk_classifiers: List[ClassifierType],
    directory="artifacts",
):

    most_important_cols = get_most_important_features(train_df_diff, n=top_k_features)

    X_train = df_train[most_important_cols]
    y_train = df_train["vuln"]

    X_test = df_test[most_important_cols]
    y_test = df_test["vuln"]

    for clf in sk_classifiers:
        classifier = sk_classifiers_map[clf]
        classifier.fit(X_train, y_train)
        y_pred = classifier.predict(X_test)
        tp = sum((y_pred == 1) & (y_test == 1))
        fp = sum((y_pred == 1) & (y_test == 0))
        tn = sum((y_pred == 0) & (y_test == 0))
        fn = sum((y_pred == 0) & (y_test == 1))
        store_classifier_info(
            directory + f"/{clf.value}.jsonl",
            clf,
            top_k_features,
            tp,
            fp,
            tn,
            fn,
            y_pred,
            y_test,
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--safe-jsonl-path",
        type=str,
        default="artifacts/accumulated_featuree_importance_safe.jsonl",
    )
    parser.add_argument(
        "--vuln-jsonl-path",
        type=str,
        default="artifacts/accumulated_featuree_importance_vuln.jsonl",
    )
    parser.add_argument(
        "--diff-jsonl-path",
        type=str,
        default="artifacts/accunuated_featuree_importance_diff.jsonl",
    )
    parser.add_argument("--output-dir", type=str, required=True, default="artifacts")
    args = parser.parse_args()

    if not os.path.exists(args.output_dir):
        logger.info("Creating output directory.", dir=args.output_dir)
        os.makedirs(args.output_dir)

    logger.info("Reading data.")
    diff_df = get_diff_data(args.diff_jsonl_path)
    train_indexes = get_training_indexes(diff_df)
    train_df_diff = diff_df.loc[train_indexes]

    df_train, df_test = get_vuln_safe_data(args.vuln_jsonl_path, args.safe_jsonl_path)

    logger.info("Training models.")
    top_k_features = np.arange(1, 100, 1)
    for k in tqdm(top_k_features):
        train_model(
            df_train,
            df_test,
            k,
            [
                ClassifierType.LOGISTIC_REGRESSION,
                ClassifierType.SVM,
                ClassifierType.KNN,
                ClassifierType.DECISION_TREE,
            ],
            args.output_dir,
        )
